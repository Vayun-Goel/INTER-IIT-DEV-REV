{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ct-ej0Oumi8"
      },
      "source": [
        "## Installing Required Dependencies through pip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEBSEXhUv1tt",
        "outputId": "0a1c1e27-2819-477e-de10-2fb93cfe7250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: discord in /usr/local/lib/python3.10/dist-packages (2.3.2)\n",
            "Requirement already satisfied: discord.py>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from discord) (2.3.2)\n",
            "Requirement already satisfied: aiohttp<4,>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from discord.py>=2.3.2->discord) (3.9.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (3.6)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install discord\n",
        "!pip install transformers\n",
        "!pip install openai\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7ruMw14uufJ"
      },
      "source": [
        "### Note that you may need to restart the system after installing dependencies\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhiY-kawvrhU"
      },
      "source": [
        "## Importing Required Dependencies, Updating TinyLlama and Downloading Important Functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZpTiW299bdW"
      },
      "outputs": [],
      "source": [
        "# Your OpenAI API Key here\n",
        "OPENAI_KEY = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_dbf4_l9fXk"
      },
      "outputs": [],
      "source": [
        "# Your Discord Bot token here\n",
        "DISCORD_BOT_TOKEN = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npw1bMou3D30"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import json\n",
        "import gdown\n",
        "import openai\n",
        "import json\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Yz6L27nW1mHM",
        "outputId": "6970c177-e507-4a1c-a56a-e26002300e23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 14540, done.\u001b[K\n",
            "remote: Counting objects: 100% (14540/14540), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4366/4366), done.\u001b[K\n",
            "remote: Total 14540 (delta 10188), reused 14335 (delta 10072), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (14540/14540), 16.56 MiB | 15.85 MiB/s, done.\n",
            "Resolving deltas: 100% (10188/10188), done.\n",
            "Already up to date.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
            "I NVCCFLAGS:  \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "rm -vrf *.o tests/*.o *.so *.dll benchmark-matmult common/build-info.cpp *.dot *.gcno tests/*.gcno *.gcda tests/*.gcda *.gcov tests/*.gcov lcov-report gcovr-report main quantize quantize-stats perplexity embedding vdot q8dot train-text-from-scratch convert-llama2c-to-ggml simple batched batched-bench save-load-state server gguf llama-bench libllava.a llava-cli baby-llama beam-search speculative infill tokenize benchmark-matmult parallel finetune export-lora lookahead tests/test-c.o metal tests/test-llama-grammar tests/test-grammar-parser tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0-llama tests/test-tokenizer-0-falcon tests/test-tokenizer-1-llama tests/test-tokenizer-1-bpe tests/test-rope tests/test-backend-ops\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
            "I NVCCFLAGS: --forward-unknown-to-host-compiler -use_fast_math -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 \n",
            "I LDFLAGS:   -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
            "\u001b[01m\u001b[Kggml.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kggml_compute_forward_scale_f32\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kggml.c:10338:22:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kdereferencing type-punned pointer will break strict-aliasing rules [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wstrict-aliasing\u0007-Wstrict-aliasing\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "10338 |     const float v = *\u001b[01;35m\u001b[K(float *) dst->op_params\u001b[m\u001b[K;\n",
            "      |                      \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kggml.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kggml_compute_backward\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kggml.c:15155:66:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kdereferencing type-punned pointer will break strict-aliasing rules [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wstrict-aliasing\u0007-Wstrict-aliasing\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "15155 |                     const float s = ((float *) tensor->op_params)\u001b[01;35m\u001b[K[\u001b[m\u001b[K0];\n",
            "      |                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c llama.cpp -o llama.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/common.cpp -o common.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/sampling.cpp -o sampling.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/grammar-parser.cpp -o grammar-parser.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/build-info.cpp -o build-info.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/console.cpp -o console.o\n",
            "nvcc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  --forward-unknown-to-host-compiler -use_fast_math -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation -Wextra-semi\" -c ggml-cuda.cu -o ggml-cuda.o\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "\u001b[01m\u001b[Kggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_scale(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, CUstream_st* const&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kggml-cuda.cu:7705:22:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kdereferencing type-punned pointer will break strict-aliasing rules [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wstrict-aliasing\u0007-Wstrict-aliasing\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 7705 |     const float scal\u001b[01;35m\u001b[Ke = ((float *) dst->op_para\u001b[m\u001b[Kms)[0];\n",
            "      |                     \u001b[01;35m\u001b[K~^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/main/main.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o main -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize/quantize.cpp build-info.o ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize-stats/quantize-stats.cpp build-info.o ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize-stats -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/perplexity/perplexity.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o perplexity -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/embedding/embedding.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o embedding -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/vdot.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o vdot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/q8dot.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o q8dot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/train.cpp -o train.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o train-text-from-scratch -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o convert-llama2c-to-ggml -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/simple/simple.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o simple -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched/batched.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched-bench/batched-bench.cpp build-info.o ggml.o llama.o common.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched-bench -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/save-load-state/save-load-state.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o save-load-state -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iexamples/server examples/server/server.cpp examples/llava/clip.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o server -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib   -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/gguf/gguf.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o gguf -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llama-bench -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llava/llava-cli.cpp examples/llava/clip.cpp examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llava-cli -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib  -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/baby-llama/baby-llama.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o baby-llama -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/beam-search/beam-search.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o beam-search -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/speculative/speculative.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o speculative -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/infill/infill.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o infill -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/tokenize/tokenize.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o tokenize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/benchmark/benchmark-matmult.cpp build-info.o ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o benchmark-matmult -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/parallel/parallel.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o parallel -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/finetune/finetune.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o finetune -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/export-lora/export-lora.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o export-lora -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookahead/lookahead.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookahead -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n",
            "Collecting numpy==1.24.4 (from -r llama.cpp/requirements.txt (line 1))\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.98 (from -r llama.cpp/requirements.txt (line 2))\n",
            "  Downloading sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from -r llama.cpp/requirements.txt (line 3)) (4.35.2)\n",
            "Collecting gguf>=0.1.0 (from -r llama.cpp/requirements.txt (line 4))\n",
            "  Downloading gguf-0.6.0-py3-none-any.whl (23 kB)\n",
            "Collecting protobuf>=4.21.0 (from -r llama.cpp/requirements.txt (line 5))\n",
            "  Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r llama.cpp/requirements.txt (line 3)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r llama.cpp/requirements.txt (line 3)) (0.19.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r llama.cpp/requirements.txt (line 3)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r llama.cpp/requirements.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r llama.cpp/requirements.txt (line 3)) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r llama.cpp/requirements.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r llama.cpp/requirements.txt (line 3)) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r llama.cpp/requirements.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r llama.cpp/requirements.txt (line 3)) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=4.34.0->-r llama.cpp/requirements.txt (line 3)) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=4.34.0->-r llama.cpp/requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r llama.cpp/requirements.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r llama.cpp/requirements.txt (line 3)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r llama.cpp/requirements.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r llama.cpp/requirements.txt (line 3)) (2023.11.17)\n",
            "Installing collected packages: sentencepiece, protobuf, numpy, gguf\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.1.99\n",
            "    Uninstalling sentencepiece-0.1.99:\n",
            "      Successfully uninstalled sentencepiece-0.1.99\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 4.25.1 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gguf-0.6.0 numpy-1.24.4 protobuf-4.25.1 sentencepiece-0.1.98\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "sentencepiece"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n",
        "!pip install -r llama.cpp/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUZen64FNF4P",
        "outputId": "68a68743-e86a-410b-e531-fb7cfd06da7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'hidelma'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 13 (delta 1), reused 0 (delta 0), pack-reused 3\u001b[K\n",
            "Unpacking objects: 100% (13/13), 478.34 KiB | 4.69 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "MODEL_ID = \"mayapati/hidelma\"\n",
        "\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/{MODEL_ID}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ8B3_G3MmC_",
        "outputId": "91ae218d-7354-400a-9de4-ba5c2104fb4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'TinyLlama-1.1B-intermediate-step-955k-token-2T'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 22 (delta 5), reused 0 (delta 0), pack-reused 3\u001b[K\n",
            "Unpacking objects: 100% (22/22), 480.55 KiB | 3.78 MiB/s, done.\n",
            "Filtering content: 100% (3/3), 201.26 MiB | 1.05 MiB/s, done.\n",
            "Encountered 2 file(s) that may not have been copied correctly on Windows:\n",
            "\tpytorch_model.bin\n",
            "\tmodel.safetensors\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ],
      "source": [
        "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-intermediate-step-955k-token-2T\"\n",
        "\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/{MODEL_ID}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNJzx67hMvG4",
        "outputId": "f9507d56-4e53-41a6-e7c4-fa70182d36b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model file hidelma/pytorch_model.bin\n",
            "params = Params(n_vocab=32000, n_embd=2048, n_layer=22, n_ctx=2048, n_ff=5632, n_head=32, n_head_kv=4, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyF16: 1>, path_model=PosixPath('hidelma'))\n",
            "32000 32000\n",
            "Vocab info: <VocabLoader with 32000 base tokens and 0 added tokens>\n",
            "Special vocab info: <SpecialVocab with 61249 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0, 'pad': 2}, add special tokens unset>\n",
            "Permuting layer 0\n",
            "Permuting layer 1\n",
            "Permuting layer 2\n",
            "Permuting layer 3\n",
            "Permuting layer 4\n",
            "Permuting layer 5\n",
            "Permuting layer 6\n",
            "Permuting layer 7\n",
            "Permuting layer 8\n",
            "Permuting layer 9\n",
            "Permuting layer 10\n",
            "Permuting layer 11\n",
            "Permuting layer 12\n",
            "Permuting layer 13\n",
            "Permuting layer 14\n",
            "Permuting layer 15\n",
            "Permuting layer 16\n",
            "Permuting layer 17\n",
            "Permuting layer 18\n",
            "Permuting layer 19\n",
            "Permuting layer 20\n",
            "Permuting layer 21\n",
            "model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32000, 2048]\n",
            "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [2048, 2048]\n",
            "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [256, 2048]\n",
            "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [256, 2048]\n",
            "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [2048, 2048]\n",
            "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [5632, 2048]\n",
            "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [5632, 2048]\n",
            "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [2048, 5632]\n",
            "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [2048]\n",
            "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [2048]\n",
            "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [2048, 2048]\n",
            "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [256, 2048]\n",
            "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [256, 2048]\n",
            "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [2048, 2048]\n",
            "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [5632, 2048]\n",
            "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [5632, 2048]\n",
            "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [2048, 5632]\n",
            "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [2048]\n",
            "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [2048]\n",
            "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [2048, 2048]\n",
            "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [256, 2048]\n",
            "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [256, 2048]\n",
            "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [2048, 2048]\n",
            "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [5632, 2048]\n",
            "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [5632, 2048]\n",
            "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [2048, 5632]\n",
            "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [2048]\n",
            "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [2048]\n",
            "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [2048, 2048]\n",
            "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [256, 2048]\n",
            "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [256, 2048]\n",
            "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [2048, 2048]\n",
            "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [5632, 2048]\n",
            "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [5632, 2048]\n",
            "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [2048, 5632]\n",
            "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [2048]\n",
            "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [2048]\n",
            "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [2048, 2048]\n",
            "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [256, 2048]\n",
            "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [256, 2048]\n",
            "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [2048, 2048]\n",
            "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [5632, 2048]\n",
            "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [5632, 2048]\n",
            "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [2048, 5632]\n",
            "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [2048]\n",
            "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [2048]\n",
            "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [2048, 2048]\n",
            "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [256, 2048]\n",
            "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [256, 2048]\n",
            "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [2048, 2048]\n",
            "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [5632, 2048]\n",
            "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [5632, 2048]\n",
            "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [2048, 5632]\n",
            "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [2048]\n",
            "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [2048]\n",
            "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [2048, 2048]\n",
            "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [256, 2048]\n",
            "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [256, 2048]\n",
            "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [2048, 2048]\n",
            "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [5632, 2048]\n",
            "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [5632, 2048]\n",
            "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [2048, 5632]\n",
            "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [2048]\n",
            "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [2048]\n",
            "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [2048, 2048]\n",
            "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [256, 2048]\n",
            "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [256, 2048]\n",
            "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [2048, 2048]\n",
            "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [5632, 2048]\n",
            "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [5632, 2048]\n",
            "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [2048, 5632]\n",
            "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [2048]\n",
            "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [2048]\n",
            "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [2048, 2048]\n",
            "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [256, 2048]\n",
            "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [256, 2048]\n",
            "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [2048, 2048]\n",
            "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [5632, 2048]\n",
            "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [5632, 2048]\n",
            "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [2048, 5632]\n",
            "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [2048]\n",
            "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [2048]\n",
            "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [2048, 2048]\n",
            "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [256, 2048]\n",
            "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [256, 2048]\n",
            "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [2048, 2048]\n",
            "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [5632, 2048]\n",
            "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [5632, 2048]\n",
            "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [2048, 5632]\n",
            "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [2048]\n",
            "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [2048]\n",
            "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [2048, 2048]\n",
            "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [256, 2048]\n",
            "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [256, 2048]\n",
            "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [2048, 2048]\n",
            "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [5632, 2048]\n",
            "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [5632, 2048]\n",
            "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [2048, 5632]\n",
            "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [2048]\n",
            "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [2048]\n",
            "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [2048, 2048]\n",
            "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [256, 2048]\n",
            "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [256, 2048]\n",
            "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [2048, 2048]\n",
            "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [5632, 2048]\n",
            "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [5632, 2048]\n",
            "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [2048, 5632]\n",
            "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [2048]\n",
            "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [2048]\n",
            "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [2048, 2048]\n",
            "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [256, 2048]\n",
            "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [256, 2048]\n",
            "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [2048, 2048]\n",
            "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [5632, 2048]\n",
            "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [5632, 2048]\n",
            "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [2048, 5632]\n",
            "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [2048]\n",
            "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [2048]\n",
            "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [2048, 2048]\n",
            "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [256, 2048]\n",
            "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [256, 2048]\n",
            "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [2048, 2048]\n",
            "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [5632, 2048]\n",
            "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [5632, 2048]\n",
            "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [2048, 5632]\n",
            "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [2048]\n",
            "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [2048]\n",
            "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [2048, 2048]\n",
            "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [256, 2048]\n",
            "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [256, 2048]\n",
            "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [2048, 2048]\n",
            "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [5632, 2048]\n",
            "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [5632, 2048]\n",
            "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [2048, 5632]\n",
            "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [2048]\n",
            "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [2048]\n",
            "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [2048, 2048]\n",
            "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [256, 2048]\n",
            "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [256, 2048]\n",
            "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [2048, 2048]\n",
            "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [5632, 2048]\n",
            "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [5632, 2048]\n",
            "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [2048, 5632]\n",
            "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [2048]\n",
            "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [2048]\n",
            "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [2048, 2048]\n",
            "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [256, 2048]\n",
            "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [256, 2048]\n",
            "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [2048, 2048]\n",
            "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [5632, 2048]\n",
            "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [5632, 2048]\n",
            "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [2048, 5632]\n",
            "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [2048]\n",
            "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [2048]\n",
            "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [2048, 2048]\n",
            "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [256, 2048]\n",
            "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [256, 2048]\n",
            "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [2048, 2048]\n",
            "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [5632, 2048]\n",
            "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [5632, 2048]\n",
            "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [2048, 5632]\n",
            "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [2048]\n",
            "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [2048]\n",
            "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [2048, 2048]\n",
            "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [256, 2048]\n",
            "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [256, 2048]\n",
            "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [2048, 2048]\n",
            "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [5632, 2048]\n",
            "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [5632, 2048]\n",
            "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [2048, 5632]\n",
            "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [2048]\n",
            "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [2048]\n",
            "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [2048, 2048]\n",
            "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [256, 2048]\n",
            "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [256, 2048]\n",
            "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [2048, 2048]\n",
            "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [5632, 2048]\n",
            "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [5632, 2048]\n",
            "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [2048, 5632]\n",
            "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [2048]\n",
            "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [2048]\n",
            "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [2048, 2048]\n",
            "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [256, 2048]\n",
            "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [256, 2048]\n",
            "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [2048, 2048]\n",
            "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [5632, 2048]\n",
            "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [5632, 2048]\n",
            "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [2048, 5632]\n",
            "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [2048]\n",
            "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [2048]\n",
            "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [2048, 2048]\n",
            "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [256, 2048]\n",
            "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [256, 2048]\n",
            "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [2048, 2048]\n",
            "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [5632, 2048]\n",
            "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [5632, 2048]\n",
            "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [2048, 5632]\n",
            "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [2048]\n",
            "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [2048]\n",
            "model.norm.weight                                -> output_norm.weight                       | F16    | [2048]\n",
            "lm_head.weight                                   -> output.weight                            | F16    | [32000, 2048]\n",
            "Writing hidelma/hidelma.fp16.bin, format 1\n",
            "gguf: This GGUF file is for Little Endian only\n",
            "gguf: Adding 61249 merge(s).\n",
            "gguf: Setting special token type bos to 1\n",
            "gguf: Setting special token type eos to 2\n",
            "gguf: Setting special token type unk to 0\n",
            "gguf: Setting special token type pad to 2\n",
            "[  1/201] Writing tensor token_embd.weight                      | size  32000 x   2048  | type F16  | T+   0\n",
            "[  2/201] Writing tensor blk.0.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   1\n",
            "[  3/201] Writing tensor blk.0.attn_k.weight                    | size    256 x   2048  | type F16  | T+   1\n",
            "[  4/201] Writing tensor blk.0.attn_v.weight                    | size    256 x   2048  | type F16  | T+   1\n",
            "[  5/201] Writing tensor blk.0.attn_output.weight               | size   2048 x   2048  | type F16  | T+   1\n",
            "[  6/201] Writing tensor blk.0.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   1\n",
            "[  7/201] Writing tensor blk.0.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   1\n",
            "[  8/201] Writing tensor blk.0.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   1\n",
            "[  9/201] Writing tensor blk.0.attn_norm.weight                 | size   2048           | type F32  | T+   1\n",
            "[ 10/201] Writing tensor blk.0.ffn_norm.weight                  | size   2048           | type F32  | T+   1\n",
            "[ 11/201] Writing tensor blk.1.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   1\n",
            "[ 12/201] Writing tensor blk.1.attn_k.weight                    | size    256 x   2048  | type F16  | T+   1\n",
            "[ 13/201] Writing tensor blk.1.attn_v.weight                    | size    256 x   2048  | type F16  | T+   1\n",
            "[ 14/201] Writing tensor blk.1.attn_output.weight               | size   2048 x   2048  | type F16  | T+   2\n",
            "[ 15/201] Writing tensor blk.1.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   2\n",
            "[ 16/201] Writing tensor blk.1.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   2\n",
            "[ 17/201] Writing tensor blk.1.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   2\n",
            "[ 18/201] Writing tensor blk.1.attn_norm.weight                 | size   2048           | type F32  | T+   2\n",
            "[ 19/201] Writing tensor blk.1.ffn_norm.weight                  | size   2048           | type F32  | T+   2\n",
            "[ 20/201] Writing tensor blk.2.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   2\n",
            "[ 21/201] Writing tensor blk.2.attn_k.weight                    | size    256 x   2048  | type F16  | T+   2\n",
            "[ 22/201] Writing tensor blk.2.attn_v.weight                    | size    256 x   2048  | type F16  | T+   2\n",
            "[ 23/201] Writing tensor blk.2.attn_output.weight               | size   2048 x   2048  | type F16  | T+   2\n",
            "[ 24/201] Writing tensor blk.2.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   2\n",
            "[ 25/201] Writing tensor blk.2.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   2\n",
            "[ 26/201] Writing tensor blk.2.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   2\n",
            "[ 27/201] Writing tensor blk.2.attn_norm.weight                 | size   2048           | type F32  | T+   2\n",
            "[ 28/201] Writing tensor blk.2.ffn_norm.weight                  | size   2048           | type F32  | T+   2\n",
            "[ 29/201] Writing tensor blk.3.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   2\n",
            "[ 30/201] Writing tensor blk.3.attn_k.weight                    | size    256 x   2048  | type F16  | T+   2\n",
            "[ 31/201] Writing tensor blk.3.attn_v.weight                    | size    256 x   2048  | type F16  | T+   3\n",
            "[ 32/201] Writing tensor blk.3.attn_output.weight               | size   2048 x   2048  | type F16  | T+   3\n",
            "[ 33/201] Writing tensor blk.3.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   3\n",
            "[ 34/201] Writing tensor blk.3.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   3\n",
            "[ 35/201] Writing tensor blk.3.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   3\n",
            "[ 36/201] Writing tensor blk.3.attn_norm.weight                 | size   2048           | type F32  | T+   3\n",
            "[ 37/201] Writing tensor blk.3.ffn_norm.weight                  | size   2048           | type F32  | T+   3\n",
            "[ 38/201] Writing tensor blk.4.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   3\n",
            "[ 39/201] Writing tensor blk.4.attn_k.weight                    | size    256 x   2048  | type F16  | T+   3\n",
            "[ 40/201] Writing tensor blk.4.attn_v.weight                    | size    256 x   2048  | type F16  | T+   3\n",
            "[ 41/201] Writing tensor blk.4.attn_output.weight               | size   2048 x   2048  | type F16  | T+   3\n",
            "[ 42/201] Writing tensor blk.4.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   3\n",
            "[ 43/201] Writing tensor blk.4.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   3\n",
            "[ 44/201] Writing tensor blk.4.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   4\n",
            "[ 45/201] Writing tensor blk.4.attn_norm.weight                 | size   2048           | type F32  | T+   4\n",
            "[ 46/201] Writing tensor blk.4.ffn_norm.weight                  | size   2048           | type F32  | T+   4\n",
            "[ 47/201] Writing tensor blk.5.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   4\n",
            "[ 48/201] Writing tensor blk.5.attn_k.weight                    | size    256 x   2048  | type F16  | T+   4\n",
            "[ 49/201] Writing tensor blk.5.attn_v.weight                    | size    256 x   2048  | type F16  | T+   4\n",
            "[ 50/201] Writing tensor blk.5.attn_output.weight               | size   2048 x   2048  | type F16  | T+   4\n",
            "[ 51/201] Writing tensor blk.5.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   4\n",
            "[ 52/201] Writing tensor blk.5.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   4\n",
            "[ 53/201] Writing tensor blk.5.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   4\n",
            "[ 54/201] Writing tensor blk.5.attn_norm.weight                 | size   2048           | type F32  | T+   4\n",
            "[ 55/201] Writing tensor blk.5.ffn_norm.weight                  | size   2048           | type F32  | T+   4\n",
            "[ 56/201] Writing tensor blk.6.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   4\n",
            "[ 57/201] Writing tensor blk.6.attn_k.weight                    | size    256 x   2048  | type F16  | T+   4\n",
            "[ 58/201] Writing tensor blk.6.attn_v.weight                    | size    256 x   2048  | type F16  | T+   4\n",
            "[ 59/201] Writing tensor blk.6.attn_output.weight               | size   2048 x   2048  | type F16  | T+   4\n",
            "[ 60/201] Writing tensor blk.6.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   4\n",
            "[ 61/201] Writing tensor blk.6.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   5\n",
            "[ 62/201] Writing tensor blk.6.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   5\n",
            "[ 63/201] Writing tensor blk.6.attn_norm.weight                 | size   2048           | type F32  | T+   5\n",
            "[ 64/201] Writing tensor blk.6.ffn_norm.weight                  | size   2048           | type F32  | T+   5\n",
            "[ 65/201] Writing tensor blk.7.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   5\n",
            "[ 66/201] Writing tensor blk.7.attn_k.weight                    | size    256 x   2048  | type F16  | T+   5\n",
            "[ 67/201] Writing tensor blk.7.attn_v.weight                    | size    256 x   2048  | type F16  | T+   5\n",
            "[ 68/201] Writing tensor blk.7.attn_output.weight               | size   2048 x   2048  | type F16  | T+   5\n",
            "[ 69/201] Writing tensor blk.7.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   5\n",
            "[ 70/201] Writing tensor blk.7.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   5\n",
            "[ 71/201] Writing tensor blk.7.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   5\n",
            "[ 72/201] Writing tensor blk.7.attn_norm.weight                 | size   2048           | type F32  | T+   5\n",
            "[ 73/201] Writing tensor blk.7.ffn_norm.weight                  | size   2048           | type F32  | T+   5\n",
            "[ 74/201] Writing tensor blk.8.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   5\n",
            "[ 75/201] Writing tensor blk.8.attn_k.weight                    | size    256 x   2048  | type F16  | T+   5\n",
            "[ 76/201] Writing tensor blk.8.attn_v.weight                    | size    256 x   2048  | type F16  | T+   5\n",
            "[ 77/201] Writing tensor blk.8.attn_output.weight               | size   2048 x   2048  | type F16  | T+   5\n",
            "[ 78/201] Writing tensor blk.8.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   5\n",
            "[ 79/201] Writing tensor blk.8.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   6\n",
            "[ 80/201] Writing tensor blk.8.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   6\n",
            "[ 81/201] Writing tensor blk.8.attn_norm.weight                 | size   2048           | type F32  | T+   6\n",
            "[ 82/201] Writing tensor blk.8.ffn_norm.weight                  | size   2048           | type F32  | T+   6\n",
            "[ 83/201] Writing tensor blk.9.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   6\n",
            "[ 84/201] Writing tensor blk.9.attn_k.weight                    | size    256 x   2048  | type F16  | T+   6\n",
            "[ 85/201] Writing tensor blk.9.attn_v.weight                    | size    256 x   2048  | type F16  | T+   6\n",
            "[ 86/201] Writing tensor blk.9.attn_output.weight               | size   2048 x   2048  | type F16  | T+   6\n",
            "[ 87/201] Writing tensor blk.9.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   8\n",
            "[ 88/201] Writing tensor blk.9.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   8\n",
            "[ 89/201] Writing tensor blk.9.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   8\n",
            "[ 90/201] Writing tensor blk.9.attn_norm.weight                 | size   2048           | type F32  | T+   9\n",
            "[ 91/201] Writing tensor blk.9.ffn_norm.weight                  | size   2048           | type F32  | T+   9\n",
            "[ 92/201] Writing tensor blk.10.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   9\n",
            "[ 93/201] Writing tensor blk.10.attn_k.weight                   | size    256 x   2048  | type F16  | T+   9\n",
            "[ 94/201] Writing tensor blk.10.attn_v.weight                   | size    256 x   2048  | type F16  | T+   9\n",
            "[ 95/201] Writing tensor blk.10.attn_output.weight              | size   2048 x   2048  | type F16  | T+   9\n",
            "[ 96/201] Writing tensor blk.10.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   9\n",
            "[ 97/201] Writing tensor blk.10.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   9\n",
            "[ 98/201] Writing tensor blk.10.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   9\n",
            "[ 99/201] Writing tensor blk.10.attn_norm.weight                | size   2048           | type F32  | T+   9\n",
            "[100/201] Writing tensor blk.10.ffn_norm.weight                 | size   2048           | type F32  | T+   9\n",
            "[101/201] Writing tensor blk.11.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   9\n",
            "[102/201] Writing tensor blk.11.attn_k.weight                   | size    256 x   2048  | type F16  | T+   9\n",
            "[103/201] Writing tensor blk.11.attn_v.weight                   | size    256 x   2048  | type F16  | T+   9\n",
            "[104/201] Writing tensor blk.11.attn_output.weight              | size   2048 x   2048  | type F16  | T+   9\n",
            "[105/201] Writing tensor blk.11.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   9\n",
            "[106/201] Writing tensor blk.11.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   9\n",
            "[107/201] Writing tensor blk.11.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   9\n",
            "[108/201] Writing tensor blk.11.attn_norm.weight                | size   2048           | type F32  | T+  10\n",
            "[109/201] Writing tensor blk.11.ffn_norm.weight                 | size   2048           | type F32  | T+  10\n",
            "[110/201] Writing tensor blk.12.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  10\n",
            "[111/201] Writing tensor blk.12.attn_k.weight                   | size    256 x   2048  | type F16  | T+  10\n",
            "[112/201] Writing tensor blk.12.attn_v.weight                   | size    256 x   2048  | type F16  | T+  10\n",
            "[113/201] Writing tensor blk.12.attn_output.weight              | size   2048 x   2048  | type F16  | T+  10\n",
            "[114/201] Writing tensor blk.12.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  10\n",
            "[115/201] Writing tensor blk.12.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  11\n",
            "[116/201] Writing tensor blk.12.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  11\n",
            "[117/201] Writing tensor blk.12.attn_norm.weight                | size   2048           | type F32  | T+  11\n",
            "[118/201] Writing tensor blk.12.ffn_norm.weight                 | size   2048           | type F32  | T+  11\n",
            "[119/201] Writing tensor blk.13.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  11\n",
            "[120/201] Writing tensor blk.13.attn_k.weight                   | size    256 x   2048  | type F16  | T+  11\n",
            "[121/201] Writing tensor blk.13.attn_v.weight                   | size    256 x   2048  | type F16  | T+  11\n",
            "[122/201] Writing tensor blk.13.attn_output.weight              | size   2048 x   2048  | type F16  | T+  11\n",
            "[123/201] Writing tensor blk.13.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  11\n",
            "[124/201] Writing tensor blk.13.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  12\n",
            "[125/201] Writing tensor blk.13.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  12\n",
            "[126/201] Writing tensor blk.13.attn_norm.weight                | size   2048           | type F32  | T+  12\n",
            "[127/201] Writing tensor blk.13.ffn_norm.weight                 | size   2048           | type F32  | T+  12\n",
            "[128/201] Writing tensor blk.14.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  14\n",
            "[129/201] Writing tensor blk.14.attn_k.weight                   | size    256 x   2048  | type F16  | T+  14\n",
            "[130/201] Writing tensor blk.14.attn_v.weight                   | size    256 x   2048  | type F16  | T+  14\n",
            "[131/201] Writing tensor blk.14.attn_output.weight              | size   2048 x   2048  | type F16  | T+  14\n",
            "[132/201] Writing tensor blk.14.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  14\n",
            "[133/201] Writing tensor blk.14.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  14\n",
            "[134/201] Writing tensor blk.14.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  14\n",
            "[135/201] Writing tensor blk.14.attn_norm.weight                | size   2048           | type F32  | T+  14\n",
            "[136/201] Writing tensor blk.14.ffn_norm.weight                 | size   2048           | type F32  | T+  14\n",
            "[137/201] Writing tensor blk.15.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  14\n",
            "[138/201] Writing tensor blk.15.attn_k.weight                   | size    256 x   2048  | type F16  | T+  14\n",
            "[139/201] Writing tensor blk.15.attn_v.weight                   | size    256 x   2048  | type F16  | T+  14\n",
            "[140/201] Writing tensor blk.15.attn_output.weight              | size   2048 x   2048  | type F16  | T+  14\n",
            "[141/201] Writing tensor blk.15.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  15\n",
            "[142/201] Writing tensor blk.15.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  15\n",
            "[143/201] Writing tensor blk.15.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  16\n",
            "[144/201] Writing tensor blk.15.attn_norm.weight                | size   2048           | type F32  | T+  16\n",
            "[145/201] Writing tensor blk.15.ffn_norm.weight                 | size   2048           | type F32  | T+  16\n",
            "[146/201] Writing tensor blk.16.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  16\n",
            "[147/201] Writing tensor blk.16.attn_k.weight                   | size    256 x   2048  | type F16  | T+  16\n",
            "[148/201] Writing tensor blk.16.attn_v.weight                   | size    256 x   2048  | type F16  | T+  16\n",
            "[149/201] Writing tensor blk.16.attn_output.weight              | size   2048 x   2048  | type F16  | T+  16\n",
            "[150/201] Writing tensor blk.16.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  16\n",
            "[151/201] Writing tensor blk.16.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  17\n",
            "[152/201] Writing tensor blk.16.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  17\n",
            "[153/201] Writing tensor blk.16.attn_norm.weight                | size   2048           | type F32  | T+  19\n",
            "[154/201] Writing tensor blk.16.ffn_norm.weight                 | size   2048           | type F32  | T+  19\n",
            "[155/201] Writing tensor blk.17.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  19\n",
            "[156/201] Writing tensor blk.17.attn_k.weight                   | size    256 x   2048  | type F16  | T+  19\n",
            "[157/201] Writing tensor blk.17.attn_v.weight                   | size    256 x   2048  | type F16  | T+  19\n",
            "[158/201] Writing tensor blk.17.attn_output.weight              | size   2048 x   2048  | type F16  | T+  19\n",
            "[159/201] Writing tensor blk.17.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  19\n",
            "[160/201] Writing tensor blk.17.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  19\n",
            "[161/201] Writing tensor blk.17.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  19\n",
            "[162/201] Writing tensor blk.17.attn_norm.weight                | size   2048           | type F32  | T+  19\n",
            "[163/201] Writing tensor blk.17.ffn_norm.weight                 | size   2048           | type F32  | T+  19\n",
            "[164/201] Writing tensor blk.18.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  19\n",
            "[165/201] Writing tensor blk.18.attn_k.weight                   | size    256 x   2048  | type F16  | T+  19\n",
            "[166/201] Writing tensor blk.18.attn_v.weight                   | size    256 x   2048  | type F16  | T+  19\n",
            "[167/201] Writing tensor blk.18.attn_output.weight              | size   2048 x   2048  | type F16  | T+  19\n",
            "[168/201] Writing tensor blk.18.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  19\n",
            "[169/201] Writing tensor blk.18.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  20\n",
            "[170/201] Writing tensor blk.18.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  20\n",
            "[171/201] Writing tensor blk.18.attn_norm.weight                | size   2048           | type F32  | T+  20\n",
            "[172/201] Writing tensor blk.18.ffn_norm.weight                 | size   2048           | type F32  | T+  20\n",
            "[173/201] Writing tensor blk.19.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  20\n",
            "[174/201] Writing tensor blk.19.attn_k.weight                   | size    256 x   2048  | type F16  | T+  20\n",
            "[175/201] Writing tensor blk.19.attn_v.weight                   | size    256 x   2048  | type F16  | T+  20\n",
            "[176/201] Writing tensor blk.19.attn_output.weight              | size   2048 x   2048  | type F16  | T+  20\n",
            "[177/201] Writing tensor blk.19.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  20\n",
            "[178/201] Writing tensor blk.19.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  20\n",
            "[179/201] Writing tensor blk.19.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  21\n",
            "[180/201] Writing tensor blk.19.attn_norm.weight                | size   2048           | type F32  | T+  21\n",
            "[181/201] Writing tensor blk.19.ffn_norm.weight                 | size   2048           | type F32  | T+  21\n",
            "[182/201] Writing tensor blk.20.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  21\n",
            "[183/201] Writing tensor blk.20.attn_k.weight                   | size    256 x   2048  | type F16  | T+  21\n",
            "[184/201] Writing tensor blk.20.attn_v.weight                   | size    256 x   2048  | type F16  | T+  21\n",
            "[185/201] Writing tensor blk.20.attn_output.weight              | size   2048 x   2048  | type F16  | T+  21\n",
            "[186/201] Writing tensor blk.20.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  21\n",
            "[187/201] Writing tensor blk.20.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  21\n",
            "[188/201] Writing tensor blk.20.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  21\n",
            "[189/201] Writing tensor blk.20.attn_norm.weight                | size   2048           | type F32  | T+  21\n",
            "[190/201] Writing tensor blk.20.ffn_norm.weight                 | size   2048           | type F32  | T+  21\n",
            "[191/201] Writing tensor blk.21.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  22\n",
            "[192/201] Writing tensor blk.21.attn_k.weight                   | size    256 x   2048  | type F16  | T+  22\n",
            "[193/201] Writing tensor blk.21.attn_v.weight                   | size    256 x   2048  | type F16  | T+  22\n",
            "[194/201] Writing tensor blk.21.attn_output.weight              | size   2048 x   2048  | type F16  | T+  22\n",
            "[195/201] Writing tensor blk.21.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  22\n",
            "[196/201] Writing tensor blk.21.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  24\n",
            "[197/201] Writing tensor blk.21.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  24\n",
            "[198/201] Writing tensor blk.21.attn_norm.weight                | size   2048           | type F32  | T+  24\n",
            "[199/201] Writing tensor blk.21.ffn_norm.weight                 | size   2048           | type F32  | T+  24\n",
            "[200/201] Writing tensor output_norm.weight                     | size   2048           | type F32  | T+  24\n",
            "[201/201] Writing tensor output.weight                          | size  32000 x   2048  | type F16  | T+  24\n",
            "Wrote hidelma/hidelma.fp16.bin\n"
          ]
        }
      ],
      "source": [
        "MODEL_ID = \"mayapati/hidelma\"\n",
        "MODEL_NAME = MODEL_ID.split('/')[-1]\n",
        "fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n",
        "!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16} --vocab-dir \"/content/TinyLlama-1.1B-intermediate-step-955k-token-2T\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4-iTHvAM0pP",
        "outputId": "6c081cfe-3ee7-4ede-a66e-762c83f6050c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "main: build = 1681 (c7e9701)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing 'hidelma/hidelma.fp16.bin' to 'hidelma/hidelma.Q5_K_M.gguf' as Q5_K_M\n",
            "llama_model_loader: loaded meta data with 21 key-value pairs and 201 tensors from hidelma/hidelma.fp16.bin (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = .\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 22\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - type  f32:   45 tensors\n",
            "llama_model_loader: - type  f16:  156 tensors\n",
            "llama_model_quantize_internal: meta size = 1708960 bytes\n",
            "[   1/ 201]                    token_embd.weight - [ 2048, 32000,     1,     1], type =    f16, quantizing to q5_K .. size =   125.00 MiB ->    42.97 MiB | hist: \n",
            "[   2/ 201]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[   3/ 201]                  blk.0.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[   4/ 201]                  blk.0.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q6_K .. size =     1.00 MiB ->     0.41 MiB | hist: \n",
            "[   5/ 201]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[   6/ 201]                blk.0.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[   7/ 201]                  blk.0.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[   8/ 201]                blk.0.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q6_K .. size =    22.00 MiB ->     9.02 MiB | hist: \n",
            "[   9/ 201]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  10/ 201]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  11/ 201]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  12/ 201]                  blk.1.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[  13/ 201]                  blk.1.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q6_K .. size =     1.00 MiB ->     0.41 MiB | hist: \n",
            "[  14/ 201]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  15/ 201]                blk.1.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  16/ 201]                  blk.1.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  17/ 201]                blk.1.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q6_K .. size =    22.00 MiB ->     9.02 MiB | hist: \n",
            "[  18/ 201]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  19/ 201]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  20/ 201]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  21/ 201]                  blk.2.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[  22/ 201]                  blk.2.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[  23/ 201]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  24/ 201]                blk.2.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  25/ 201]                  blk.2.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  26/ 201]                blk.2.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  27/ 201]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  28/ 201]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  29/ 201]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  30/ 201]                  blk.3.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[  31/ 201]                  blk.3.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[  32/ 201]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  33/ 201]                blk.3.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  34/ 201]                  blk.3.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  35/ 201]                blk.3.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  36/ 201]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  37/ 201]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  38/ 201]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  39/ 201]                  blk.4.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[  40/ 201]                  blk.4.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q6_K .. size =     1.00 MiB ->     0.41 MiB | hist: \n",
            "[  41/ 201]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  42/ 201]                blk.4.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  43/ 201]                  blk.4.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  44/ 201]                blk.4.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q6_K .. size =    22.00 MiB ->     9.02 MiB | hist: \n",
            "[  45/ 201]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  46/ 201]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  47/ 201]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  48/ 201]                  blk.5.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[  49/ 201]                  blk.5.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[  50/ 201]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  51/ 201]                blk.5.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  52/ 201]                  blk.5.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  53/ 201]                blk.5.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  54/ 201]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  55/ 201]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  56/ 201]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  57/ 201]                  blk.6.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[  58/ 201]                  blk.6.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[  59/ 201]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  60/ 201]                blk.6.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  61/ 201]                  blk.6.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  62/ 201]                blk.6.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  63/ 201]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  64/ 201]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  65/ 201]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  66/ 201]                  blk.7.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[  67/ 201]                  blk.7.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q6_K .. size =     1.00 MiB ->     0.41 MiB | hist: \n",
            "[  68/ 201]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  69/ 201]                blk.7.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  70/ 201]                  blk.7.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  71/ 201]                blk.7.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q6_K .. size =    22.00 MiB ->     9.02 MiB | hist: \n",
            "[  72/ 201]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  73/ 201]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  74/ 201]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  75/ 201]                  blk.8.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[  76/ 201]                  blk.8.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[  77/ 201]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  78/ 201]                blk.8.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  79/ 201]                  blk.8.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  80/ 201]                blk.8.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  81/ 201]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  82/ 201]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  83/ 201]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  84/ 201]                  blk.9.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[  85/ 201]                  blk.9.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[  86/ 201]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  87/ 201]                blk.9.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  88/ 201]                  blk.9.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  89/ 201]                blk.9.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  90/ 201]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  91/ 201]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  92/ 201]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  93/ 201]                 blk.10.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[  94/ 201]                 blk.10.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q6_K .. size =     1.00 MiB ->     0.41 MiB | hist: \n",
            "[  95/ 201]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  96/ 201]               blk.10.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  97/ 201]                 blk.10.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[  98/ 201]               blk.10.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q6_K .. size =    22.00 MiB ->     9.02 MiB | hist: \n",
            "[  99/ 201]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 100/ 201]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 101/ 201]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 102/ 201]                 blk.11.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[ 103/ 201]                 blk.11.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[ 104/ 201]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 105/ 201]               blk.11.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 106/ 201]                 blk.11.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 107/ 201]               blk.11.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 108/ 201]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 109/ 201]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 110/ 201]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 111/ 201]                 blk.12.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[ 112/ 201]                 blk.12.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[ 113/ 201]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 114/ 201]               blk.12.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 115/ 201]                 blk.12.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 116/ 201]               blk.12.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 117/ 201]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 118/ 201]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 119/ 201]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 120/ 201]                 blk.13.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[ 121/ 201]                 blk.13.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q6_K .. size =     1.00 MiB ->     0.41 MiB | hist: \n",
            "[ 122/ 201]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 123/ 201]               blk.13.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 124/ 201]                 blk.13.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 125/ 201]               blk.13.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q6_K .. size =    22.00 MiB ->     9.02 MiB | hist: \n",
            "[ 126/ 201]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 127/ 201]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 128/ 201]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 129/ 201]                 blk.14.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[ 130/ 201]                 blk.14.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[ 131/ 201]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 132/ 201]               blk.14.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 133/ 201]                 blk.14.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 134/ 201]               blk.14.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 135/ 201]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 136/ 201]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 137/ 201]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 138/ 201]                 blk.15.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[ 139/ 201]                 blk.15.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[ 140/ 201]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 141/ 201]               blk.15.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 142/ 201]                 blk.15.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 143/ 201]               blk.15.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 144/ 201]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 145/ 201]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 146/ 201]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 147/ 201]                 blk.16.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[ 148/ 201]                 blk.16.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q6_K .. size =     1.00 MiB ->     0.41 MiB | hist: \n",
            "[ 149/ 201]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 150/ 201]               blk.16.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 151/ 201]                 blk.16.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 152/ 201]               blk.16.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q6_K .. size =    22.00 MiB ->     9.02 MiB | hist: \n",
            "[ 153/ 201]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 154/ 201]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 155/ 201]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 156/ 201]                 blk.17.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[ 157/ 201]                 blk.17.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[ 158/ 201]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 159/ 201]               blk.17.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 160/ 201]                 blk.17.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 161/ 201]               blk.17.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 162/ 201]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 163/ 201]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 164/ 201]                 blk.18.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 165/ 201]                 blk.18.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[ 166/ 201]                 blk.18.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[ 167/ 201]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 168/ 201]               blk.18.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 169/ 201]                 blk.18.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 170/ 201]               blk.18.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 171/ 201]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 172/ 201]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 173/ 201]                 blk.19.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 174/ 201]                 blk.19.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[ 175/ 201]                 blk.19.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q6_K .. size =     1.00 MiB ->     0.41 MiB | hist: \n",
            "[ 176/ 201]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 177/ 201]               blk.19.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 178/ 201]                 blk.19.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 179/ 201]               blk.19.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q6_K .. size =    22.00 MiB ->     9.02 MiB | hist: \n",
            "[ 180/ 201]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 181/ 201]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 182/ 201]                 blk.20.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 183/ 201]                 blk.20.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[ 184/ 201]                 blk.20.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q6_K .. size =     1.00 MiB ->     0.41 MiB | hist: \n",
            "[ 185/ 201]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 186/ 201]               blk.20.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 187/ 201]                 blk.20.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 188/ 201]               blk.20.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q6_K .. size =    22.00 MiB ->     9.02 MiB | hist: \n",
            "[ 189/ 201]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 190/ 201]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 191/ 201]                 blk.21.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 192/ 201]                 blk.21.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q5_K .. size =     1.00 MiB ->     0.34 MiB | hist: \n",
            "[ 193/ 201]                 blk.21.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q6_K .. size =     1.00 MiB ->     0.41 MiB | hist: \n",
            "[ 194/ 201]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 195/ 201]               blk.21.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 196/ 201]                 blk.21.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q5_K .. size =    22.00 MiB ->     7.56 MiB | hist: \n",
            "[ 197/ 201]               blk.21.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q6_K .. size =    22.00 MiB ->     9.02 MiB | hist: \n",
            "[ 198/ 201]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 199/ 201]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 200/ 201]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 201/ 201]                        output.weight - [ 2048, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   125.00 MiB ->    51.27 MiB | hist: \n",
            "llama_model_quantize_internal: model size  =  2098.35 MB\n",
            "llama_model_quantize_internal: quant size  =   745.11 MB\n",
            "\n",
            "main: quantize time = 120793.73 ms\n",
            "main:    total time = 120793.73 ms\n"
          ]
        }
      ],
      "source": [
        "QUANTIZATION_METHODS = [\"q5_k_m\"]\n",
        "\n",
        "for method in QUANTIZATION_METHODS:\n",
        "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n",
        "    !./llama.cpp/quantize {fp16} {qtype} {method}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBeo0LgbM-r3",
        "outputId": "34e49491-f83d-4abf-dbac-3a8147d241e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.24.tar.gz (8.8 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/8.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/8.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/8.8 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m8.4/8.8 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.24.4)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.24-cp310-cp310-manylinux_2_35_x86_64.whl size=2087880 sha256=d25ac24aae55b96f0aa88e2d074c5f311a1a43227e52fe736b6b2b8e1bdf918d\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/5f/31/172f820a61d82eb7b057b7d05ff9a05345012066e8ae781788\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.2.24\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fPzomRWhNfLr",
        "outputId": "6ccd5c14-a9ca-4e2e-94eb-9994313621c7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'final_tool_database.json'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Retriver_model_url='https://drive.google.com/drive/folders/11DeDLA5nWss5CWU5-bvHmw77bhXlFrNO'\n",
        "gdown.download_folder(Retriver_model_url, quiet=True, use_cookies=False)\n",
        "gdown.download(\"https://drive.google.com/uc?id=1AKHYcS-bFnpeN_f2X57LBW7EhaRuVXiA\", quiet=True, use_cookies=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmNOCW-zxSZ5"
      },
      "source": [
        "## Defining builder functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LETnbiKHWMee"
      },
      "outputs": [],
      "source": [
        "from transformers import StoppingCriteria\n",
        "import torch\n",
        "\n",
        "class EosListStoppingCriteria(StoppingCriteria):\n",
        "    def _init_(self, eos_sequence = [518, 29914, 25580, 29962]):\n",
        "        self.eos_sequence = eos_sequence\n",
        "\n",
        "    def _call_(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        last_ids = input_ids[:,-len(self.eos_sequence):].tolist()\n",
        "        return self.eos_sequence in last_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OC1c59guWOD4"
      },
      "outputs": [],
      "source": [
        "def get_embeddings(texts):\n",
        "    response = openai.Embedding.create(\n",
        "        model=\"text-embedding-ada-002\",\n",
        "        input=texts\n",
        "    )\n",
        "    return [embedding['embedding'] for embedding in response['data']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2JH5yuI3VvT",
        "outputId": "781ed801-69cc-49d4-8ee1-43b28e38717c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Make sure to change the path according to work environment. The paths should \"Current_Directory/checkpoint_epoch_4.pt\" and \"Current_Directory/hidelma/hidelma.Q5_K_M.gguf\"\n",
        "model = SentenceTransformer('/content/checkpoint_epoch_4.pt')\n",
        "LLM = Llama(model_path=\"/content/hidelma/hidelma.Q5_K_M.gguf\", stopping_criteria = [EosListStoppingCriteria()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhRtFENHyatg"
      },
      "source": [
        "## Start of AI-Agent-007 ToolBot Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiKU28Ak7XVd"
      },
      "source": [
        "### Example to improve GPT Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI9WOP4d16mJ"
      },
      "outputs": [],
      "source": [
        "example=\"\"\"\n",
        "example query 1: Prioritize my P0 issues and add them to the current sprint\n",
        "\n",
        "THOUGHT_ACTION_PAIR_1:\n",
        "thought: The user query is asking to prioritize user's P0 issues and add them to the current sprint, I will first call the tool \"whoami\" to get the user ID.\n",
        "action: Using \"whoami\" to return the ID of the current user.\n",
        "JSON:\n",
        "{\n",
        "    \"tool_name\": \"whoami\",\n",
        "    \"arguments\": []\n",
        "}\n",
        "\n",
        "THOUGHT_ACTION_PAIR_2:\n",
        "thought: Next I will call the \"work_list\" tool to find the P0 issue of the current user.\n",
        "action: Using the \"work_list\" tool.\n",
        "JSON:\n",
        "{\n",
        "    \"tool_name\": \"works_list\",\n",
        "    \"arguments\":\n",
        "    [\n",
        "        {\n",
        "            \"argument_name\": \"issue.priority\",\n",
        "            \"argument_value\": [\"p0\"]\n",
        "        },\n",
        "        {\n",
        "            \"argument_name\": \"owned_by\",\n",
        "            \"argument_value\": [\"$$PREV[0]\"]\n",
        "        },\n",
        "        {\n",
        "            \"argument_name\": \"type\",\n",
        "            \"argument_value\": [\"issue\"]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "THOUGHT_ACTION_PAIR_3:\n",
        "thought: Then to prioritize the P0 issues, I will call the \"prioritize_objects\" tool.\n",
        "action: Check if the 'prioritize_objects' tool is available.\n",
        "JSON:\n",
        "{\n",
        "    \"tool_name\": \"prioritize_objects\",\n",
        "    \"arguments\":\n",
        "    [\n",
        "        {\n",
        "            \"argument_name\": \"objects\",\n",
        "            \"argument_value\": \"$$PREV[1]\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "THOUGHT_ACTION_PAIR_4:\n",
        "thought: Now to get the current sprint ID, I will use \"get_sprint_id\".\n",
        "action: The 'get_sprint_id' tool can be used to retrieve the current sprint ID.\n",
        "JSON:\n",
        "{\n",
        "    \"tool_name\": \"get_sprint_id\",\n",
        "    \"arguments\": []\n",
        "}\n",
        "\n",
        "THOUGHT_ACTION_PAIR_5:\n",
        "thought: Then to add the work items to the current sprint, i call 'add_work_items_to_sprint'.\n",
        "action: Check if the 'add_work_items_to_sprint' tool is available.\n",
        "JSON:\n",
        "{\n",
        "  \"tool_name\": \"add_work_items_to_sprint\",\n",
        "  \"arguments\": [\n",
        "    {\n",
        "      \"argument_name\": \"work_ids\",\n",
        "      \"argument_value\": \"$$PREV[2]\"\n",
        "    },\n",
        "    {\n",
        "      \"argument_name\": \"sprint_id\",\n",
        "      \"argument_value\": \"$$PREV[3]\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "Expected_output:\n",
        "[\n",
        "    {\n",
        "        \"tool_name\": \"whoami\",\n",
        "        \"arguments\": []\n",
        "    },\n",
        "    {\n",
        "        \"tool_name\": \"works_list\",\n",
        "        \"arguments\":\n",
        "        [\n",
        "            {\n",
        "                \"argument_name\": \"issue.priority\",\n",
        "                \"argument_value\": [\"p0\"]\n",
        "            },\n",
        "            {\n",
        "                \"argument_name\": \"owned_by\",\n",
        "                \"argument_value\": [\"$$PREV[0]\"]\n",
        "            },\n",
        "            {\n",
        "                \"argument_name\": \"type\",\n",
        "                \"argument_value\": [\"issue\"]\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"tool_name\": \"prioritize_objects\",\n",
        "        \"arguments\":\n",
        "        [\n",
        "            {\n",
        "                \"argument_name\": \"objects\",\n",
        "                \"argument_value\": \"$$PREV[1]\"\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"tool_name\": \"get_sprint_id\",\n",
        "        \"arguments\": []\n",
        "    },\n",
        "    {\n",
        "        \"tool_name\": \"add_work_items_to_sprint\",\n",
        "        \"arguments\":\n",
        "        [\n",
        "            {\n",
        "                \"argument_name\": \"work_ids\",\n",
        "                \"argument_value\": \"$$PREV[2]\"\n",
        "            },\n",
        "            {\n",
        "                \"argument_name\": \"sprint_id\",\n",
        "                \"argument_value\": \"$$PREV[3]\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "END_JSON\n",
        "**************************************************\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpNKq79x8TEL"
      },
      "source": [
        "## Regex Parser for extracting precise JSON from Chain of Thoughts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yF9UD_hjcCwe"
      },
      "outputs": [],
      "source": [
        "# def extract_json_from_string(input_string):\n",
        "#     import json\n",
        "#     import re\n",
        "\n",
        "#     match = re.search(r'output:', input_string, re.IGNORECASE)\n",
        "#     if match:\n",
        "#         start_pos = match.end()\n",
        "#         try:\n",
        "#             json_data = input_string[start_pos:].strip()\n",
        "#             parsed_json = json.loads(json_data)\n",
        "#             return json.dumps(parsed_json, indent=4)\n",
        "#         except json.JSONDecodeError:\n",
        "#             return \"Invalid JSON format in the input string.\"\n",
        "#     else:\n",
        "#         return \"No JSON data found in the input string.\"\n",
        "\n",
        "\n",
        "\n",
        "def extract_json_with_fallback(input_string):\n",
        "    match_start = re.search(r'output:', input_string, re.IGNORECASE)\n",
        "    match_end = re.search(r'END_JSON', input_string, re.IGNORECASE)\n",
        "\n",
        "    if match_start:\n",
        "        start_pos = match_start.end()\n",
        "        if match_end:\n",
        "            end_pos = match_end.start()\n",
        "            json_data = input_string[start_pos:end_pos].strip()\n",
        "        else:\n",
        "            json_data = input_string[start_pos:].strip()\n",
        "\n",
        "        try:\n",
        "            parsed_json = json.loads(json_data)\n",
        "            return json.dumps(parsed_json, indent=4)\n",
        "        except json.JSONDecodeError:\n",
        "            return \"Invalid JSON format in the input string.\"\n",
        "    else:\n",
        "        return \"No 'output:' marker found in the input string.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDTOLalz8b32"
      },
      "source": [
        "## Extractor Function for removing TinyLlama cache and generating precise Chain of Thought (COT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sQuO0EUKwOU"
      },
      "outputs": [],
      "source": [
        "def extract_string(input_string):\n",
        "    inst1_index = input_string.find(\"[/INST]\")\n",
        "    inst2_index = input_string.find(\" <</INST>>\")\n",
        "\n",
        "    if inst1_index == -1 and inst2_index == -1:\n",
        "        return input_string\n",
        "\n",
        "    if inst1_index == -1 or (inst2_index != -1 and inst2_index < inst1_index):\n",
        "        return input_string[:inst2_index]\n",
        "    else:\n",
        "        return input_string[:inst1_index]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvp4Wvde9Ip-"
      },
      "source": [
        "## ToolBot hosted on our very own Discord Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRS7-xqzv5aQ",
        "outputId": "f22566ca-d888-4e1b-883a-f9fb5b5df834"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[30;1m2023-12-22 07:15:11\u001b[0m \u001b[34;1mINFO    \u001b[0m \u001b[35mdiscord.client\u001b[0m logging in using static token\n",
            "\u001b[30;1m2023-12-22 07:15:11\u001b[0m \u001b[34;1mINFO    \u001b[0m \u001b[35mdiscord.client\u001b[0m logging in using static token\n",
            "\u001b[30;1m2023-12-22 07:15:11\u001b[0m \u001b[34;1mINFO    \u001b[0m \u001b[35mdiscord.client\u001b[0m logging in using static token\n",
            "\u001b[30;1m2023-12-22 07:15:11\u001b[0m \u001b[34;1mINFO    \u001b[0m \u001b[35mdiscord.client\u001b[0m logging in using static token\n",
            "\u001b[30;1m2023-12-22 07:15:11\u001b[0m \u001b[34;1mINFO    \u001b[0m \u001b[35mdiscord.client\u001b[0m logging in using static token\n",
            "INFO:discord.client:logging in using static token\n",
            "\u001b[30;1m2023-12-22 07:15:11\u001b[0m \u001b[34;1mINFO    \u001b[0m \u001b[35mdiscord.gateway\u001b[0m Shard ID None has connected to Gateway (Session ID: d4b64d6e209df8112a5c440a318349a1).\n",
            "\u001b[30;1m2023-12-22 07:15:11\u001b[0m \u001b[34;1mINFO    \u001b[0m \u001b[35mdiscord.gateway\u001b[0m Shard ID None has connected to Gateway (Session ID: d4b64d6e209df8112a5c440a318349a1).\n",
            "\u001b[30;1m2023-12-22 07:15:11\u001b[0m \u001b[34;1mINFO    \u001b[0m \u001b[35mdiscord.gateway\u001b[0m Shard ID None has connected to Gateway (Session ID: d4b64d6e209df8112a5c440a318349a1).\n",
            "\u001b[30;1m2023-12-22 07:15:11\u001b[0m \u001b[34;1mINFO    \u001b[0m \u001b[35mdiscord.gateway\u001b[0m Shard ID None has connected to Gateway (Session ID: d4b64d6e209df8112a5c440a318349a1).\n",
            "\u001b[30;1m2023-12-22 07:15:11\u001b[0m \u001b[34;1mINFO    \u001b[0m \u001b[35mdiscord.gateway\u001b[0m Shard ID None has connected to Gateway (Session ID: d4b64d6e209df8112a5c440a318349a1).\n",
            "INFO:discord.gateway:Shard ID None has connected to Gateway (Session ID: d4b64d6e209df8112a5c440a318349a1).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged in as ToolBot59DevRev\n",
            "[{'tool_name': 'works_list', 'api_description': 'A flexible tool to fetch a customized list of work items, allowing users to apply various filters such as parts, user creation, priorities, organizational association, ownership, stages, response requirements, and severity levels. It supports an array of argument types for intricate filtering and a limit feature for controlling the output size. Ideal for streamlined task and ticket management in diverse organizational contexts.', 'required_parameters': [{'name': 'applies_to_part', 'type': 'array of strings', 'description': 'Selects work items associated with any of the specified parts.'}, {'name': 'created_by', 'type': 'array of strings', 'description': 'Targets work items created by the listed users.'}, {'name': 'issue.priority', 'type': 'array of strings', 'description': 'Focuses on issues of specified priorities, such as p0, p1, p2, p3.'}, {'name': 'issue.rev_orgs', 'type': 'array of strings', 'description': 'Narrows down issues related to certain Rev organizations.'}, {'name': 'limit', 'type': 'integer (int32)', 'description': 'Sets the maximum number of work items to be retrieved, with a default of 50.'}, {'name': 'owned_by', 'type': 'array of strings', 'description': 'Filters work items based on specified owner users.'}, {'name': 'stage.name', 'type': 'array of strings', 'description': 'Selects work items in specified stages.'}, {'name': 'ticket.needs_response', 'type': 'boolean', 'description': 'Finds tickets requiring a response.'}, {'name': 'ticket.rev_org', 'type': 'array of strings', 'description': 'Filters tickets associated with given Rev organizations.'}, {'name': 'ticket.severity', 'type': 'array of strings', 'description': 'Targets tickets of specific severities like blocker, high, medium, or low.'}, {'name': 'ticket.source_channel', 'type': 'array of strings', 'description': 'Selects tickets originating from specified source channels.'}, {'name': 'type', 'type': 'array of strings', 'description': 'Filters work items by type, such as issue, ticket, or task.'}]}, {'tool_name': 'get_similar_work_items', 'api_description': \"Efficiently locates work items similar to a given reference item. By providing a work item's ID, users can quickly identify related items, enhancing task correlation and project management.\", 'required_parameters': [{'name': 'work_id', 'type': 'string', 'description': 'The identifier of the work item to find similarities with.'}]}, {'tool_name': 'create_actionable_tasks_from_text', 'api_description': 'Transforms textual input into actionable tasks by analyzing the text to extract insights and generate corresponding tasks. Ideal for converting written content into structured, actionable workflows.', 'required_parameters': [{'name': 'text', 'type': 'string', 'description': 'The text to analyze for generating actionable insights and tasks.'}]}, {'tool_name': 'add_work_items_to_sprint', 'api_description': 'Facilitates the addition of multiple work items to a specified sprint. Users can provide a list of work item IDs and a sprint ID to efficiently assign tasks to a particular sprint cycle.', 'required_parameters': [{'name': 'work_ids', 'type': 'array of strings', 'description': 'List of IDs for work items to be added to the sprint.'}, {'name': 'sprint_id', 'type': 'string', 'description': 'The unique identifier of the sprint to add work items to.'}]}, {'tool_name': 'get_sprint_id', 'api_description': 'Retrieves the ID of the current sprint. This tool is essential for users needing to quickly identify the sprint they are currently working in.', 'required_parameters': []}]\n",
            "#########Given the user query \"Given a customer meeting transcript T, create action items and add them to my current sprint,\" let's break it down into subproblems and create a sequence of thought-action pairs with JSON API calls.\n",
            "\n",
            "THOUGHT_ACTION_PAIR_1:\n",
            "thought: The user query involves analyzing a transcript to create action items. I need to use the \"create_actionable_tasks_from_text\" tool to process the transcript text and generate tasks.\n",
            "action: Call the \"create_actionable_tasks_from_text\" tool with the transcript text.\n",
            "JSON:\n",
            "{\n",
            "    \"tool_name\": \"create_actionable_tasks_from_text\",\n",
            "    \"arguments\": [\n",
            "        {\n",
            "            \"argument_name\": \"text\",\n",
            "            \"argument_value\": \"T\"\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "\n",
            "THOUGHT_ACTION_PAIR_2:\n",
            "thought: After creating action items, I need to find the ID of the current sprint to add these items to it. The \"get_sprint_id\" tool will provide this information.\n",
            "action: Call the \"get_sprint_id\" tool to retrieve the current sprint ID.\n",
            "JSON:\n",
            "{\n",
            "    \"tool_name\": \"get_sprint_id\",\n",
            "    \"arguments\": []\n",
            "}\n",
            "\n",
            "THOUGHT_ACTION_PAIR_3:\n",
            "thought: With the action items created and the current sprint ID obtained, I can now add these items to the sprint using the \"add_work_items_to_sprint\" tool.\n",
            "action: Call the \"add_work_items_to_sprint\" tool with the list of work item IDs and the sprint ID.\n",
            "JSON:\n",
            "{\n",
            "    \"tool_name\": \"add_work_items_to_sprint\",\n",
            "    \"arguments\": [\n",
            "        {\n",
            "            \"argument_name\": \"work_ids\",\n",
            "            \"argument_value\": \"$$PREV[0]\"\n",
            "        },\n",
            "        {\n",
            "            \"argument_name\": \"sprint_id\",\n",
            "            \"argument_value\": \"$$PREV[1]\"\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "\n",
            "Final_output:\n",
            "[\n",
            "    {\n",
            "        \"tool_name\": \"create_actionable_tasks_from_text\",\n",
            "        \"arguments\": [\n",
            "            {\n",
            "                \"argument_name\": \"text\",\n",
            "                \"argument_value\": \"T\"\n",
            "            }\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"tool_name\": \"get_sprint_id\",\n",
            "        \"arguments\": []\n",
            "    },\n",
            "    {\n",
            "        \"tool_name\": \"add_work_items_to_sprint\",\n",
            "        \"arguments\": [\n",
            "            {\n",
            "                \"argument_name\": \"work_ids\",\n",
            "                \"argument_value\": \"$$PREV[0]\"\n",
            "            },\n",
            "            {\n",
            "                \"argument_name\": \"sprint_id\",\n",
            "                \"argument_value\": \"$$PREV[1]\"\n",
            "            }\n",
            "        ]\n",
            "    }\n",
            "]\n",
            "\n",
            "Each thought-action pair assumes that the tool execution is possible based on the user query and the available tools. The argument values are strings, as specified in the user query, and match the required argument types for the tools. The logic for the arguments follows the descriptions provided for each tool. The output from one tool is used as input for the subsequent tools, maintaining the correct sequence and data types.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[30;1m2023-12-22 07:16:02\u001b[0m \u001b[33;1mWARNING \u001b[0m \u001b[35mdiscord.gateway\u001b[0m Shard ID None heartbeat blocked for more than 10 seconds.\n",
            "Loop thread traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 129, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-22-bd7cb102d7c5>\", line 310, in <cell line: 307>\n",
            "    bot.run(DISCORD_BOT_TOKEN)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/client.py\", line 860, in run\n",
            "    asyncio.run(runner())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 31, in run\n",
            "    return loop.run_until_complete(task)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 93, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 129, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 315, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 205, in step\n",
            "    step_orig(task, exc)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/client.py\", line 441, in _run_event\n",
            "    await coro(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/bot.py\", line 1395, in on_message\n",
            "    await self.process_commands(message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/bot.py\", line 1392, in process_commands\n",
            "    await self.invoke(ctx)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/bot.py\", line 1350, in invoke\n",
            "    await ctx.command.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/core.py\", line 1029, in invoke\n",
            "    await injected(*ctx.args, **ctx.kwargs)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/core.py\", line 235, in wrapped\n",
            "    ret = await coro(*args, **kwargs)\n",
            "  File \"<ipython-input-22-bd7cb102d7c5>\", line 274, in talk\n",
            "    response_file_path = await generate_response(prompt[2:]+ \"Previous Response:\"+str(full_conversation))\n",
            "  File \"<ipython-input-22-bd7cb102d7c5>\", line 214, in generate_response\n",
            "    time.sleep(6)\n",
            "\n",
            "\u001b[30;1m2023-12-22 07:16:02\u001b[0m \u001b[33;1mWARNING \u001b[0m \u001b[35mdiscord.gateway\u001b[0m Shard ID None heartbeat blocked for more than 10 seconds.\n",
            "Loop thread traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 129, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-22-bd7cb102d7c5>\", line 310, in <cell line: 307>\n",
            "    bot.run(DISCORD_BOT_TOKEN)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/client.py\", line 860, in run\n",
            "    asyncio.run(runner())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 31, in run\n",
            "    return loop.run_until_complete(task)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 93, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 129, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 315, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 205, in step\n",
            "    step_orig(task, exc)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/client.py\", line 441, in _run_event\n",
            "    await coro(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/bot.py\", line 1395, in on_message\n",
            "    await self.process_commands(message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/bot.py\", line 1392, in process_commands\n",
            "    await self.invoke(ctx)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/bot.py\", line 1350, in invoke\n",
            "    await ctx.command.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/core.py\", line 1029, in invoke\n",
            "    await injected(*ctx.args, **ctx.kwargs)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/core.py\", line 235, in wrapped\n",
            "    ret = await coro(*args, **kwargs)\n",
            "  File \"<ipython-input-22-bd7cb102d7c5>\", line 274, in talk\n",
            "    response_file_path = await generate_response(prompt[2:]+ \"Previous Response:\"+str(full_conversation))\n",
            "  File \"<ipython-input-22-bd7cb102d7c5>\", line 214, in generate_response\n",
            "    time.sleep(6)\n",
            "\n",
            "\u001b[30;1m2023-12-22 07:16:02\u001b[0m \u001b[33;1mWARNING \u001b[0m \u001b[35mdiscord.gateway\u001b[0m Shard ID None heartbeat blocked for more than 10 seconds.\n",
            "Loop thread traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 129, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-22-bd7cb102d7c5>\", line 310, in <cell line: 307>\n",
            "    bot.run(DISCORD_BOT_TOKEN)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/client.py\", line 860, in run\n",
            "    asyncio.run(runner())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 31, in run\n",
            "    return loop.run_until_complete(task)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 93, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 129, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 315, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 205, in step\n",
            "    step_orig(task, exc)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/client.py\", line 441, in _run_event\n",
            "    await coro(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/bot.py\", line 1395, in on_message\n",
            "    await self.process_commands(message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/bot.py\", line 1392, in process_commands\n",
            "    await self.invoke(ctx)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/bot.py\", line 1350, in invoke\n",
            "    await ctx.command.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/core.py\", line 1029, in invoke\n",
            "    await injected(*ctx.args, **ctx.kwargs)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/core.py\", line 235, in wrapped\n",
            "    ret = await coro(*args, **kwargs)\n",
            "  File \"<ipython-input-22-bd7cb102d7c5>\", line 274, in talk\n",
            "    response_file_path = await generate_response(prompt[2:]+ \"Previous Response:\"+str(full_conversation))\n",
            "  File \"<ipython-input-22-bd7cb102d7c5>\", line 214, in generate_response\n",
            "    time.sleep(6)\n",
            "\n",
            "\u001b[30;1m2023-12-22 07:16:02\u001b[0m \u001b[33;1mWARNING \u001b[0m \u001b[35mdiscord.gateway\u001b[0m Shard ID None heartbeat blocked for more than 10 seconds.\n",
            "Loop thread traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 129, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-22-bd7cb102d7c5>\", line 310, in <cell line: 307>\n",
            "    bot.run(DISCORD_BOT_TOKEN)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/client.py\", line 860, in run\n",
            "    asyncio.run(runner())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 31, in run\n",
            "    return loop.run_until_complete(task)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 93, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 129, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 315, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 205, in step\n",
            "    step_orig(task, exc)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/client.py\", line 441, in _run_event\n",
            "    await coro(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/bot.py\", line 1395, in on_message\n",
            "    await self.process_commands(message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/bot.py\", line 1392, in process_commands\n",
            "    await self.invoke(ctx)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/bot.py\", line 1350, in invoke\n",
            "    await ctx.command.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/core.py\", line 1029, in invoke\n",
            "    await injected(*ctx.args, **ctx.kwargs)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/core.py\", line 235, in wrapped\n",
            "    ret = await coro(*args, **kwargs)\n",
            "  File \"<ipython-input-22-bd7cb102d7c5>\", line 274, in talk\n",
            "    response_file_path = await generate_response(prompt[2:]+ \"Previous Response:\"+str(full_conversation))\n",
            "  File \"<ipython-input-22-bd7cb102d7c5>\", line 214, in generate_response\n",
            "    time.sleep(6)\n",
            "\n",
            "\u001b[30;1m2023-12-22 07:16:02\u001b[0m \u001b[33;1mWARNING \u001b[0m \u001b[35mdiscord.gateway\u001b[0m Shard ID None heartbeat blocked for more than 10 seconds.\n",
            "Loop thread traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 129, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-22-bd7cb102d7c5>\", line 310, in <cell line: 307>\n",
            "    bot.run(DISCORD_BOT_TOKEN)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/client.py\", line 860, in run\n",
            "    asyncio.run(runner())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 31, in run\n",
            "    return loop.run_until_complete(task)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 93, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 129, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 315, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 205, in step\n",
            "    step_orig(task, exc)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/client.py\", line 441, in _run_event\n",
            "    await coro(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/bot.py\", line 1395, in on_message\n",
            "    await self.process_commands(message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/bot.py\", line 1392, in process_commands\n",
            "    await self.invoke(ctx)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/bot.py\", line 1350, in invoke\n",
            "    await ctx.command.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/core.py\", line 1029, in invoke\n",
            "    await injected(*ctx.args, **ctx.kwargs)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/core.py\", line 235, in wrapped\n",
            "    ret = await coro(*args, **kwargs)\n",
            "  File \"<ipython-input-22-bd7cb102d7c5>\", line 274, in talk\n",
            "    response_file_path = await generate_response(prompt[2:]+ \"Previous Response:\"+str(full_conversation))\n",
            "  File \"<ipython-input-22-bd7cb102d7c5>\", line 214, in generate_response\n",
            "    time.sleep(6)\n",
            "\n",
            "WARNING:discord.gateway:Shard ID None heartbeat blocked for more than 10 seconds.\n",
            "Loop thread traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 129, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-22-bd7cb102d7c5>\", line 310, in <cell line: 307>\n",
            "    bot.run(DISCORD_BOT_TOKEN)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/client.py\", line 860, in run\n",
            "    asyncio.run(runner())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 31, in run\n",
            "    return loop.run_until_complete(task)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 93, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 129, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 315, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 205, in step\n",
            "    step_orig(task, exc)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/client.py\", line 441, in _run_event\n",
            "    await coro(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/bot.py\", line 1395, in on_message\n",
            "    await self.process_commands(message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/bot.py\", line 1392, in process_commands\n",
            "    await self.invoke(ctx)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/bot.py\", line 1350, in invoke\n",
            "    await ctx.command.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/core.py\", line 1029, in invoke\n",
            "    await injected(*ctx.args, **ctx.kwargs)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/ext/commands/core.py\", line 235, in wrapped\n",
            "    ret = await coro(*args, **kwargs)\n",
            "  File \"<ipython-input-22-bd7cb102d7c5>\", line 274, in talk\n",
            "    response_file_path = await generate_response(prompt[2:]+ \"Previous Response:\"+str(full_conversation))\n",
            "  File \"<ipython-input-22-bd7cb102d7c5>\", line 214, in generate_response\n",
            "    time.sleep(6)\n",
            "\n",
            "\u001b[30;1m2023-12-22 07:16:12\u001b[0m \u001b[31mERROR   \u001b[0m \u001b[35mdiscord.client\u001b[0m Attempting a reconnect in 0.61s\n",
            "\u001b[31mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/client.py\", line 659, in connect\n",
            "    await self.ws.poll_event()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/gateway.py\", line 646, in poll_event\n",
            "    raise ConnectionClosed(self.socket, shard_id=self.shard_id, code=code) from None\n",
            "discord.errors.ConnectionClosed: Shard ID None WebSocket closed with 1000\u001b[0m\n",
            "\u001b[30;1m2023-12-22 07:16:12\u001b[0m \u001b[31mERROR   \u001b[0m \u001b[35mdiscord.client\u001b[0m Attempting a reconnect in 0.61s\n",
            "\u001b[31mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/client.py\", line 659, in connect\n",
            "    await self.ws.poll_event()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/gateway.py\", line 646, in poll_event\n",
            "    raise ConnectionClosed(self.socket, shard_id=self.shard_id, code=code) from None\n",
            "discord.errors.ConnectionClosed: Shard ID None WebSocket closed with 1000\u001b[0m\n",
            "\u001b[30;1m2023-12-22 07:16:12\u001b[0m \u001b[31mERROR   \u001b[0m \u001b[35mdiscord.client\u001b[0m Attempting a reconnect in 0.61s\n",
            "\u001b[31mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/client.py\", line 659, in connect\n",
            "    await self.ws.poll_event()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/gateway.py\", line 646, in poll_event\n",
            "    raise ConnectionClosed(self.socket, shard_id=self.shard_id, code=code) from None\n",
            "discord.errors.ConnectionClosed: Shard ID None WebSocket closed with 1000\u001b[0m\n",
            "\u001b[30;1m2023-12-22 07:16:12\u001b[0m \u001b[31mERROR   \u001b[0m \u001b[35mdiscord.client\u001b[0m Attempting a reconnect in 0.61s\n",
            "\u001b[31mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/client.py\", line 659, in connect\n",
            "    await self.ws.poll_event()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/gateway.py\", line 646, in poll_event\n",
            "    raise ConnectionClosed(self.socket, shard_id=self.shard_id, code=code) from None\n",
            "discord.errors.ConnectionClosed: Shard ID None WebSocket closed with 1000\u001b[0m\n",
            "\u001b[30;1m2023-12-22 07:16:12\u001b[0m \u001b[31mERROR   \u001b[0m \u001b[35mdiscord.client\u001b[0m Attempting a reconnect in 0.61s\n",
            "\u001b[31mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/client.py\", line 659, in connect\n",
            "    await self.ws.poll_event()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/gateway.py\", line 646, in poll_event\n",
            "    raise ConnectionClosed(self.socket, shard_id=self.shard_id, code=code) from None\n",
            "discord.errors.ConnectionClosed: Shard ID None WebSocket closed with 1000\u001b[0m\n",
            "ERROR:discord.client:Attempting a reconnect in 0.61s\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/client.py\", line 659, in connect\n",
            "    await self.ws.poll_event()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/discord/gateway.py\", line 646, in poll_event\n",
            "    raise ConnectionClosed(self.socket, shard_id=self.shard_id, code=code) from None\n",
            "discord.errors.ConnectionClosed: Shard ID None WebSocket closed with 1000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "############yes\n",
            "{1072768125327659040: [{'role': 'user', 'content': ' Given a customer meeting transcript T, create action items and add them to my current sprint'}, {'role': 'system', 'content': ''}]}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[30;1m2023-12-22 07:16:12\u001b[0m \u001b[34;1mINFO    \u001b[0m \u001b[35mdiscord.gateway\u001b[0m Shard ID None has successfully RESUMED session d4b64d6e209df8112a5c440a318349a1.\n",
            "\u001b[30;1m2023-12-22 07:16:12\u001b[0m \u001b[34;1mINFO    \u001b[0m \u001b[35mdiscord.gateway\u001b[0m Shard ID None has successfully RESUMED session d4b64d6e209df8112a5c440a318349a1.\n",
            "\u001b[30;1m2023-12-22 07:16:12\u001b[0m \u001b[34;1mINFO    \u001b[0m \u001b[35mdiscord.gateway\u001b[0m Shard ID None has successfully RESUMED session d4b64d6e209df8112a5c440a318349a1.\n",
            "\u001b[30;1m2023-12-22 07:16:12\u001b[0m \u001b[34;1mINFO    \u001b[0m \u001b[35mdiscord.gateway\u001b[0m Shard ID None has successfully RESUMED session d4b64d6e209df8112a5c440a318349a1.\n",
            "\u001b[30;1m2023-12-22 07:16:12\u001b[0m \u001b[34;1mINFO    \u001b[0m \u001b[35mdiscord.gateway\u001b[0m Shard ID None has successfully RESUMED session d4b64d6e209df8112a5c440a318349a1.\n",
            "INFO:discord.gateway:Shard ID None has successfully RESUMED session d4b64d6e209df8112a5c440a318349a1.\n"
          ]
        }
      ],
      "source": [
        "import discord\n",
        "from discord.ext import commands\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import openai\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "intents = discord.Intents.default()\n",
        "intents.messages = True\n",
        "intents.message_content = True\n",
        "\n",
        "is_processing_request = False\n",
        "\n",
        "bot = commands.Bot(command_prefix=\"!\", intents=intents)\n",
        "\n",
        "openai.api_key = OPENAI_KEY\n",
        "\n",
        "conversation_history = {}\n",
        "\n",
        "def recursive_call(response,query,relevant_api):\n",
        "  retry = f\"\"\"\n",
        "  This is the {response} for the following Query: {query} and these are the tools : {relevant_api}.\n",
        "   Generate an alternative response which solves the query. Follow the same structure as the initial response: {response}.\n",
        "  \"\"\"\n",
        "\n",
        "  recursive_response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4-1106-preview\",\n",
        "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                      {\"role\": \"user\", \"content\": retry}],\n",
        "            temperature = 0.2\n",
        "        )\n",
        "  time.sleep(6)\n",
        "  query_recursive = f'''\n",
        "    For this User Query: {query}\n",
        "    we now have the following response:\n",
        "    {recursive_response.choices[0].message['content']}\n",
        "\n",
        "    Now analyse this response and check whether the generated APIs are actually present in relevant_api:{relevant_api}.\n",
        "    If you find that some APIs are not present in {relevant_api} or you think the response generated does not solve the problem return \"NO\" else return \"YES\". Make sure to answer in one word.\n",
        "    '''\n",
        "  time.sleep(6)\n",
        "  response2 = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4-1106-preview\",\n",
        "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                      {\"role\": \"user\", \"content\": query_recursive}],\n",
        "            temperature = 0.2\n",
        "        )\n",
        "\n",
        "\n",
        "  if (response2.choices[0].message[\"content\"].lower())[0] == \"y\":\n",
        "            return recursive_response.choices[0].message['content']\n",
        "  else:\n",
        "          return \"[]\"\n",
        "\n",
        "@bot.event\n",
        "async def on_ready():\n",
        "    print(f\"Logged in as {bot.user.name}\")\n",
        "\n",
        "async def write_response_to_file(response, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        file.write(response)\n",
        "\n",
        "async def generate_response(prompt):\n",
        "    try:\n",
        "            query = prompt\n",
        "\n",
        "\n",
        "            system_message_cot = \"Given a user query in English, you will respond with a well-reasoned response\"\n",
        "            prompt_cot = f\"[INST] <<SYS>>\\n{system_message_cot}\\n<</SYS>>\\n\\n {query} [/INST]\"\n",
        "            # output_cot = LLM(prompt)\n",
        "\n",
        "            # input_str = \"This is a sample string [/INST] with some text. <</INST>> More text here.\"\n",
        "            # result_cot = extract_string(output_cot[\"choices\"][0]['text'])\n",
        "\n",
        "            query_api_description = []\n",
        "\n",
        "            \"\"\"\n",
        "            This code is for combining a self-trained IR Bert Retriever. Use this if you want more generalised approach\n",
        "            towards tool-retrieval. However it has been noticed that ada-002(OpenAI Embeddings) performs better in terms\n",
        "            of focused set of tools for better accuracy.\n",
        "            \"\"\"\n",
        "            '''\n",
        "\n",
        "            final_ = []\n",
        "            rel_tools = []\n",
        "            with open('/content/final_tool_database.json', 'r') as file:\n",
        "                json_list = json.load(file)\n",
        "            for tool in json_list:\n",
        "              query_api_description += [[query,tool[\"api_description\"]]]\n",
        "\n",
        "\n",
        "            for i in range(len(query_api_description)):\n",
        "              encodings = [model.encode(query_api_description[i])]\n",
        "              similarity = cosine_similarity(encodings[0][0].reshape(1,-1), encodings[0][1].reshape(1,-1))\n",
        "              final_ += [((similarity[0][0]),(json_list[i][\"tool_name\"]))]\n",
        "\n",
        "            sorted_data = sorted(final_, key=lambda x: x[0],reverse=True)\n",
        "\n",
        "            for i in range(0,min(len(sorted_data),10)):\n",
        "              rel_tools+=[sorted_data[i][1]]\n",
        "\n",
        "            final_list = []\n",
        "\n",
        "            for tool in json_list:\n",
        "                tool_name = tool[\"tool_name\"]\n",
        "                if tool_name in rel_tools:\n",
        "                  final_list+=[tool]\n",
        "\n",
        "            relevant_api = f\"\"\"{final_list}\n",
        "            \"\"\"\n",
        "\n",
        "            '''\n",
        "\n",
        "            final_ = []\n",
        "            rel_tools = []\n",
        "            with open('/content/final_tool_database.json', 'r') as file:\n",
        "                json_list = json.load(file)\n",
        "\n",
        "            query_api_description = []\n",
        "            for tool in json_list:\n",
        "                query_api_description.append([query, tool[\"api_description\"]])\n",
        "\n",
        "            all_texts = [item for sublist in query_api_description for item in sublist]\n",
        "            embeddings = get_embeddings(all_texts)\n",
        "\n",
        "            for i in range(0, len(query_api_description), 2):\n",
        "                similarity = cosine_similarity(\n",
        "                    [embeddings[i]], [embeddings[i + 1]]\n",
        "                )\n",
        "                final_.append((similarity[0][0], json_list[i//2][\"tool_name\"]))\n",
        "\n",
        "            sorted_data = sorted(final_, key=lambda x: x[0], reverse=True)\n",
        "            rel_tools = [tool[1] for tool in sorted_data[:min(10, len(sorted_data))]]\n",
        "\n",
        "            final_list = [tool for tool in json_list if tool[\"tool_name\"] in rel_tools]\n",
        "\n",
        "            relevant_api = f\"{final_list}\"\n",
        "            print(relevant_api)\n",
        "            system_prompt = f\"\"\"Given a user query and an available list of relevant tools with the tool arguments.\n",
        "        a- Dissect the user query into smaller, manageable subproblems that lead towards a solution.\n",
        "        b- For each subproblem, create a sequence of thoughts, actions, and JSON API calls. Analyze each step: If a subproblem cannot be solved with the available tools, return \"not possible.\"\n",
        "          remember\n",
        "        c- the Final_output should be the list of all the JSON in the previous steps if you can solve the problem otherwise it should be an empty list\n",
        "          format: Final_output=[{{....}},{{....}}.....] or Final_output=[] depending on whether you can solve the question or not\n",
        "          the Final_output list should be non-empty if you fully solve the problem\n",
        "        1. For each thought-action pair, analyze if the tool execution will be possible or not. Assume the output for each thought-action using the given list of tools.\n",
        "        2. Match the argument_value given by you with the specified argument type for the tool. If the argument values and argument types match then assume that the function call is possible and will be successful, otherwise end the thought-action sequence and return not possible.\n",
        "          remember that you shouldn't change any data type of the query all the items in user queries are strings\n",
        "          and the output of the API you can assume using the description whether it is an object or a string or any other\n",
        "        3. If the argument_values and argument_type matches then check the logic for the argument, the argument_value should be according to the argument_description. If the argument values and argument types match then assume that the function call is possible and will be successful, otherwise end the thought-action sequence and return not possible.\n",
        "\n",
        "        For each thought-action pair generate the following:\n",
        "        JSON\n",
        "\n",
        "        “tool_name”: name of the TOOL from the TOOL list.\n",
        "        “arguments”: [\n",
        "        {{\n",
        "            “argument_name”: Name of TOOL’s argument,\n",
        "            “argument_value”: Value that the argument takes according to the user query. It should have the correct Argument type. Fill according to the argument description and use the Argument Value Example for sample help\n",
        "        }}]\n",
        "        4. After the generation of thought-action JSON, try to analyze whether the subproblem\n",
        "        5. The output is a list of argument names called in a combination. Output from one argument can be given as input to other arguments in the tool. To reference the value of the ith argument in the chain, use $$PREV[i] as argument value. i = 0, 1, .. j-1; j = current arguments’s index in the array. This should be correct according to the thought-action sequence.\n",
        "        6. One tool can be used multiple times in the same output with different argument values\n",
        "        7. Do not hallucinate new argument names. The argument value should be in specified format for that argument only\n",
        "        8. If you are going to use a previous value as an argument then check whether the type of the argument is accepted by the argument_value you are using it in\n",
        "        9. If some information regarding me is required then you can use the tool \"who_am_i\" if it is available in the list of tools. If it is not present DO NOT USE IT.\n",
        "        10. Give me the sequence of thought-action JSON pairs in the following format for the ith thought-action pair\n",
        "        THOUGHT_ACTION_PAIR_i: {{\n",
        "            thought:  ,\n",
        "            2-action: ,\n",
        "            3-JSON: ,\n",
        "            4-analysis:\n",
        "        }}\n",
        "\n",
        "        11. If some information about a 'customer' is asked then you need to use tools which can provide information about a customer for ex \"search_object_by_name\". You can use output of that tool for other tools later.\n",
        "        12. If the current sprint is required then use the tool \"get_sprint_id\" if it is available in the given set of tools. If it is not present then DO NOT use it. If a similar tool is present you can use that\n",
        "        13.If the question is a physchological, philosophical or general knowledge question not related to an organisation then return an empty list: [] and give Expected_output as \"ORG_NOT_POSSIBLE\"\n",
        "        Final output given by you should be a list of thought and action pairs in natural language, response for the thought-action pair in the JSON schema specified, if response is not possible by matching argument types with the type of argument value returned by you or the argument_value is not matching the argument_description then return NOT_POSSIBLE.\n",
        "        Return the final json after \"Expected_output\" and add \"END_JSON\" after final output json is complete. THIS IS VERY IMPORTANT\n",
        "        REMEMBER:- you have to use the tools name given below only you cannot generate your new tools.\n",
        "        Relevant Tools List:\n",
        "        {relevant_api}\n",
        "          [if no tool present in the list then you ARE not required to use them]\n",
        "        User Query: {query}\n",
        "        i am giving you a set of example you can refer them to get the basic stucture of  what kind of thinking process to adopt.YOU CAN ONLY TAKE INSPIRATION DO NOT COPY THEM THESE ARE ONLY FOR YOUR REFERENCE!!\n",
        "        examples:{example}\n",
        "        Generate output using only the given tools for the User Query. DO NOT HALLUCINATE NEW TOOLS, ONLY USE THE TOOLS PROVIDED\n",
        "\n",
        "        \"\"\"\n",
        "# You also can refer to a Guide:{result_cot}\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-4-1106-preview\",\n",
        "                messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                          {\"role\": \"user\", \"content\": system_prompt}],\n",
        "                temperature = 0.2\n",
        "            )\n",
        "\n",
        "            response_content = response.choices[0].message['content']\n",
        "\n",
        "            response_content_no_spaces = response_content.replace(\" \", \"\")\n",
        "            contains_org_not_possible = \"ORG_NOT_POSSIBLE\" in response_content_no_spaces\n",
        "            if(contains_org_not_possible):\n",
        "              print(\"YAYA\")\n",
        "              await write_response_to_file(\"[]\", '/content/response.txt')\n",
        "              return '/content/response.txt'\n",
        "\n",
        "            print(\"#########\"+response_content)\n",
        "\n",
        "            time.sleep(6)\n",
        "\n",
        "            query2 = f'''\n",
        "        For this User Query: {query}\n",
        "        we now have the following response to the query:\n",
        "        {response.choices[0].message['content']}\n",
        "\n",
        "        Now analyse this response and check whether the generated APIs are actually present in relevant_api:{relevant_api}.\n",
        "        If you find that APIs are not present in {relevant_api} or you think the answer does not solve the problem return \"NO\" else return \"YES\". Make sure to answer in one word.\n",
        "        '''\n",
        "            response2 = openai.ChatCompletion.create(\n",
        "                model=\"gpt-4-1106-preview\",\n",
        "                messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                          {\"role\": \"user\", \"content\": query2}],\n",
        "                temperature = 0.2\n",
        "            )\n",
        "            time.sleep(6)\n",
        "            print(\"############\"+response2.choices[0].message[\"content\"].lower())\n",
        "            if (response2.choices[0].message[\"content\"].lower())[0] == \"y\":\n",
        "              if(response.choices[0].message['content']==\"[]\"):\n",
        "                await write_response_to_file(\"[]\", '/content/response.txt')\n",
        "              else:\n",
        "                output = extract_json_from_string(response.choices[0].message['content'])\n",
        "                await write_response_to_file(output, '/content/response.txt')\n",
        "\n",
        "              return '/content/response.txt'\n",
        "            else:\n",
        "              x = recursive_call(response.choices[0].message['content'],query,relevant_api)\n",
        "              if(x==\"[]\"):\n",
        "                await write_response_to_file(x, '/content/response.txt')\n",
        "              else:\n",
        "                output = extract_json_from_string(x)\n",
        "                await write_response_to_file(x, '/content/response.txt')\n",
        "\n",
        "              return '/content/response.txt'\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return \"An error occurred while generating the response.\"\n",
        "\n",
        "@bot.command()\n",
        "async def talk(ctx, *, prompt):\n",
        "    global is_processing_request\n",
        "    global conversation_history\n",
        "\n",
        "    if is_processing_request:\n",
        "        return\n",
        "    is_processing_request = True\n",
        "\n",
        "    user_id = ctx.message.author.id\n",
        "    if user_id not in conversation_history:\n",
        "        conversation_history[user_id] = []\n",
        "\n",
        "    try:\n",
        "        check = prompt.lower()[0:3]\n",
        "        if(check.startswith(\"q:\")):\n",
        "\n",
        "            full_conversation = conversation_history[user_id][-2:]\n",
        "            processing_message = await ctx.send(\"Processing your request...\")\n",
        "            response_file_path = await generate_response(prompt[2:]+ \"Previous Response:\"+str(full_conversation))\n",
        "\n",
        "            with open(response_file_path, 'rb') as file:\n",
        "                await ctx.send(file=discord.File(file, 'response.txt'))\n",
        "                bot_response = file.read().decode('utf-8')\n",
        "\n",
        "            conversation_history[user_id].append({\"role\": \"user\", \"content\": prompt[2:]})\n",
        "            conversation_history[user_id].append({\"role\": \"system\", \"content\": bot_response})\n",
        "\n",
        "            print(conversation_history)\n",
        "            await processing_message.delete()\n",
        "        else:\n",
        "            full_conversation = conversation_history[user_id][-2:]\n",
        "            processing_message = await ctx.send(\"Processing your request...\")\n",
        "\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-4-1106-preview\",\n",
        "                messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"+ \"Previous Response:\"+str(full_conversation)}] ,\n",
        "                temperature=0.2\n",
        "            )\n",
        "\n",
        "            bot_response = response.choices[0].message['content']\n",
        "            conversation_history[user_id].append({\"role\": \"system\", \"content\": bot_response})\n",
        "            print(conversation_history)\n",
        "\n",
        "            await processing_message.edit(content=bot_response)\n",
        "\n",
        "    except Exception as e:\n",
        "        await ctx.send(f\"An error occurred: {e}\")\n",
        "    finally:\n",
        "        is_processing_request = False\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  loop = asyncio.get_event_loop()\n",
        "  try:\n",
        "      bot.run(DISCORD_BOT_TOKEN)\n",
        "  except KeyboardInterrupt:\n",
        "      loop.run_until_complete(bot.logout())\n",
        "      loop.close()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}